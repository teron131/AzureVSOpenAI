{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def stylistic_features(text):\n",
    "    \"\"\"\n",
    "    Extracts stylistic features from a given text, including text length.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text from which stylistic features are extracted.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the sentence count, average word length, and text length.\n",
    "    \"\"\"\n",
    "    text_length = len(text)\n",
    "    sentences = text.split(\".\")\n",
    "    sentence_count = len(sentences) - 1\n",
    "    word_lengths = [len(word) for word in text.split()]\n",
    "    average_word_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
    "\n",
    "    return {\n",
    "        \"text_length\": text_length,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"average_word_length\": average_word_length,\n",
    "    }\n",
    "\n",
    "\n",
    "def stylistic_similarity(\n",
    "    text1,\n",
    "    text2,\n",
    "    features_to_compare=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the stylistic similarity between two texts based on customizable features, including consideration of text length.\n",
    "\n",
    "    Parameters:\n",
    "    - text1 (str): The first text.\n",
    "    - text2 (str): The second text.\n",
    "    - features_to_compare (list, optional): A list of features to compare for similarity. Options include 'sentence_count', 'average_word_length', and 'text_length'. Defaults to comparing all features.\n",
    "\n",
    "    Returns:\n",
    "    - float: The overall stylistic similarity between the two texts based on the selected features.\n",
    "    \"\"\"\n",
    "    if features_to_compare is None:\n",
    "        features_to_compare = [\"text_length\", \"sentence_count\", \"average_word_length\"]\n",
    "\n",
    "    features1 = stylistic_features(text1)\n",
    "    features2 = stylistic_features(text2)\n",
    "    similarities = []\n",
    "\n",
    "    for feature in features_to_compare:\n",
    "        if feature in features1 and feature in features2:\n",
    "            similarity = 1 - abs(features1[feature] - features2[feature]) / max(features1[feature], features2[feature], 1)\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    if similarities:\n",
    "        average_similarity = np.mean(similarities)\n",
    "    else:\n",
    "        average_similarity = 0\n",
    "\n",
    "    return average_similarity\n",
    "\n",
    "\n",
    "def extract_structural_features(text):\n",
    "    \"\"\"\n",
    "    Extracts structural features from a given text, including headers, bullet points, and numbered lists.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text from which structural features are extracted.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the header count, bullet points count, and numbered list count.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    # Count headers more robustly by considering all levels of markdown headers\n",
    "    features[\"header_count\"] = sum(1 for line in text.split(\"\\n\") if line.strip().startswith(\"#\"))\n",
    "    features[\"bullet_point_count\"] = text.count(\"\\n- \") + text.count(\"\\n* \")\n",
    "\n",
    "    # Enhanced handling for numbered lists including first and second tier (e.g., 1., a.)\n",
    "    def is_numbered_list_item(line):\n",
    "        parts = line.strip().split(\". \", 1)\n",
    "        if len(parts) == 2:\n",
    "            first_part, _ = parts\n",
    "            return first_part.isdigit() or (first_part.isalpha() and len(first_part) == 1)\n",
    "        return False\n",
    "\n",
    "    features[\"numbered_list_count\"] = sum(1 for line in text.split(\"\\n\") if is_numbered_list_item(line))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def structural_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculates the structural similarity between two texts, considering headers, bullet points, and numbered lists.\n",
    "    Adjusts calculation based on the presence of structural features, rather than using a simple average.\n",
    "\n",
    "    Parameters:\n",
    "    - text1 (str): The first text.\n",
    "    - text2 (str): The second text.\n",
    "\n",
    "    Returns:\n",
    "    - float: The overall structural similarity between the two texts.\n",
    "    \"\"\"\n",
    "    features1 = extract_structural_features(text1)\n",
    "    features2 = extract_structural_features(text2)\n",
    "\n",
    "    similarities = []\n",
    "    weights = []\n",
    "\n",
    "    # Calculate header similarity if headers are present in either text\n",
    "    if features1[\"header_count\"] > 0 or features2[\"header_count\"] > 0:\n",
    "        header_similarity = 1 - abs(features1[\"header_count\"] - features2[\"header_count\"]) / max(features1[\"header_count\"], features2[\"header_count\"], 1)\n",
    "        similarities.append(header_similarity)\n",
    "        weights.append(max(features1[\"header_count\"], features2[\"header_count\"]))\n",
    "\n",
    "    # Calculate bullet point similarity if bullet points are present in either text\n",
    "    if features1[\"bullet_point_count\"] > 0 or features2[\"bullet_point_count\"] > 0:\n",
    "        bullet_similarity = 1 - abs(features1[\"bullet_point_count\"] - features2[\"bullet_point_count\"]) / max(features1[\"bullet_point_count\"], features2[\"bullet_point_count\"], 1)\n",
    "        similarities.append(bullet_similarity)\n",
    "        weights.append(max(features1[\"bullet_point_count\"], features2[\"bullet_point_count\"]))\n",
    "\n",
    "    # Calculate numbered list similarity if numbered lists are present in either text\n",
    "    if features1[\"numbered_list_count\"] > 0 or features2[\"numbered_list_count\"] > 0:\n",
    "        numbered_list_similarity = 1 - abs(features1[\"numbered_list_count\"] - features2[\"numbered_list_count\"]) / max(features1[\"numbered_list_count\"], features2[\"numbered_list_count\"], 1)\n",
    "        similarities.append(numbered_list_similarity)\n",
    "        weights.append(max(features1[\"numbered_list_count\"], features2[\"numbered_list_count\"]))\n",
    "\n",
    "    # Calculate weighted average of similarities\n",
    "    if similarities:\n",
    "        weighted_average_similarity = sum(similarity * weight for similarity, weight in zip(similarities, weights)) / sum(weights)\n",
    "    else:\n",
    "        weighted_average_similarity = 1\n",
    "\n",
    "    return weighted_average_similarity\n",
    "\n",
    "\n",
    "def format_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculates a comprehensive format similarity between two texts, combining stylistic and structural scores, including the consideration of text length.\n",
    "\n",
    "    Parameters:\n",
    "    - text1 (str): The first text.\n",
    "    - text2 (str): The second text.\n",
    "\n",
    "    Returns:\n",
    "    - float: The overall format similarity between the two texts.\n",
    "    \"\"\"\n",
    "    features_to_compare = [\"text_length\", \"sentence_count\", \"average_word_length\"]\n",
    "    stylistic_score = stylistic_similarity(text1, text2, features_to_compare[:])\n",
    "    structural_score = structural_similarity(text1, text2)\n",
    "\n",
    "    overall_score = stylistic_score * 0.3 + structural_score * 0.7\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def BERTsimilarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two texts using BERT embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - text1 (str): The first text.\n",
    "    - text2 (str): The second text.\n",
    "\n",
    "    Returns:\n",
    "    - float: The similarity between the two texts.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    embedding1 = model.encode(text1)\n",
    "    embedding2 = model.encode(text2)\n",
    "    similarity = 1 - cosine(embedding1, embedding2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the JSON structure is a list of dictionaries with keys: question, model, provider, response\n",
    "# Load the JSON data\n",
    "with open(\"responses.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Calculate similarities\n",
    "# Initialize a column for similarities\n",
    "df[\"BERT_similarity\"] = 0.0\n",
    "df[\"format_similarity\"] = 0.0\n",
    "\n",
    "# Iterate over each question and model to calculate similarities between OpenAI and Azure responses\n",
    "for question in df[\"question\"].unique():\n",
    "    for model in df[\"model\"].unique():\n",
    "        openai_response = df[(df[\"question\"] == question) & (df[\"model\"] == model) & (df[\"provider\"] == \"openai\")][\"response\"].iloc[0]\n",
    "        azure_response = df[(df[\"question\"] == question) & (df[\"model\"] == model) & (df[\"provider\"] == \"azure\")][\"response\"].iloc[0]\n",
    "\n",
    "        # Update the DataFrame with the calculated similaritys\n",
    "        df.loc[(df[\"question\"] == question) & (df[\"model\"] == model), \"BERT_similarity\"] = BERTsimilarity(openai_response, azure_response)\n",
    "        df.loc[(df[\"question\"] == question) & (df[\"model\"] == model), \"format_similarity\"] = format_similarity(openai_response, azure_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model</th>\n",
       "      <th>BERT_similarity</th>\n",
       "      <th>format_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me some useful info and tips about AI.</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.846239</td>\n",
       "      <td>0.538891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you aware of Phantom Liberty? Please brief...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.790551</td>\n",
       "      <td>0.977796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nCyberpunk 2077 — Never Fade Away by P. T. Ad...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.933726</td>\n",
       "      <td>0.972131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nA bird in the hand is worth two in the bush\\...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.913258</td>\n",
       "      <td>0.257918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nProve or disprove: “An individual is risk av...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.924695</td>\n",
       "      <td>0.482897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explain gradient descent.</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.838881</td>\n",
       "      <td>0.220882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Describe Sagittarius A* and TON 618.</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.880041</td>\n",
       "      <td>0.978038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tell me some useful info and tips about AI.</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.892335</td>\n",
       "      <td>0.914315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Are you aware of Phantom Liberty? Please brief...</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.862975</td>\n",
       "      <td>0.951599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nCyberpunk 2077 — Never Fade Away by P. T. Ad...</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.887388</td>\n",
       "      <td>0.786608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\nA bird in the hand is worth two in the bush\\...</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.909161</td>\n",
       "      <td>0.423542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\nProve or disprove: “An individual is risk av...</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.953328</td>\n",
       "      <td>0.711078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Explain gradient descent.</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.944355</td>\n",
       "      <td>0.978119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Describe Sagittarius A* and TON 618.</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.877477</td>\n",
       "      <td>0.979215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question        model  \\\n",
       "0         Tell me some useful info and tips about AI.        gpt-4   \n",
       "1   Are you aware of Phantom Liberty? Please brief...        gpt-4   \n",
       "2   \\nCyberpunk 2077 — Never Fade Away by P. T. Ad...        gpt-4   \n",
       "3   \\nA bird in the hand is worth two in the bush\\...        gpt-4   \n",
       "4   \\nProve or disprove: “An individual is risk av...        gpt-4   \n",
       "5                           Explain gradient descent.        gpt-4   \n",
       "6                Describe Sagittarius A* and TON 618.        gpt-4   \n",
       "7         Tell me some useful info and tips about AI.  gpt-4-turbo   \n",
       "8   Are you aware of Phantom Liberty? Please brief...  gpt-4-turbo   \n",
       "9   \\nCyberpunk 2077 — Never Fade Away by P. T. Ad...  gpt-4-turbo   \n",
       "10  \\nA bird in the hand is worth two in the bush\\...  gpt-4-turbo   \n",
       "11  \\nProve or disprove: “An individual is risk av...  gpt-4-turbo   \n",
       "12                          Explain gradient descent.  gpt-4-turbo   \n",
       "13               Describe Sagittarius A* and TON 618.  gpt-4-turbo   \n",
       "\n",
       "    BERT_similarity  format_similarity  \n",
       "0          0.846239           0.538891  \n",
       "1          0.790551           0.977796  \n",
       "2          0.933726           0.972131  \n",
       "3          0.913258           0.257918  \n",
       "4          0.924695           0.482897  \n",
       "5          0.838881           0.220882  \n",
       "6          0.880041           0.978038  \n",
       "7          0.892335           0.914315  \n",
       "8          0.862975           0.951599  \n",
       "9          0.887388           0.786608  \n",
       "10         0.909161           0.423542  \n",
       "11         0.953328           0.711078  \n",
       "12         0.944355           0.978119  \n",
       "13         0.877477           0.979215  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by removing specific columns and duplicates, and resetting the index.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=[\"provider\", \"response\"])\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.sort_values([\"model\"])\n",
    "\n",
    "    # Use factorize to encode the unique questions, starting with 1\n",
    "    # df[\"question\"], _ = pd.factorize(df[\"question\"])\n",
    "    # Increment by 1 to start numbering from 1 instead of 0\n",
    "    # df[\"question\"] = df[\"question\"] + 1\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_similarity = clean_data(df.copy(deep=True))\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14.000000\n",
       "mean      0.889601\n",
       "std       0.045067\n",
       "min       0.790551\n",
       "25%       0.866600\n",
       "50%       0.889862\n",
       "75%       0.921836\n",
       "max       0.953328\n",
       "Name: BERT_similarity, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[\"BERT_similarity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.000000\n",
       "mean     0.875341\n",
       "std      0.052710\n",
       "min      0.790551\n",
       "25%      0.842560\n",
       "50%      0.880041\n",
       "75%      0.918976\n",
       "max      0.933726\n",
       "Name: BERT_similarity, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[df_similarity[\"model\"] == \"gpt-4\"][\"BERT_similarity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.000000\n",
       "mean     0.903860\n",
       "std      0.033879\n",
       "min      0.862975\n",
       "25%      0.882433\n",
       "50%      0.892335\n",
       "75%      0.926758\n",
       "max      0.953328\n",
       "Name: BERT_similarity, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[df_similarity[\"model\"] == \"gpt-4o\"][\"BERT_similarity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14.000000\n",
       "mean      0.726645\n",
       "std       0.286258\n",
       "min       0.220882\n",
       "25%       0.496896\n",
       "50%       0.850462\n",
       "75%       0.976380\n",
       "max       0.979215\n",
       "Name: format_similarity, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[\"format_similarity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.000000\n",
       "mean     0.632651\n",
       "std      0.340309\n",
       "min      0.220882\n",
       "25%      0.370408\n",
       "50%      0.538891\n",
       "75%      0.974964\n",
       "max      0.978038\n",
       "Name: format_similarity, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[df_similarity[\"model\"] == \"gpt-4\"][\"format_similarity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.000000\n",
       "mean     0.820640\n",
       "std      0.202781\n",
       "min      0.423542\n",
       "25%      0.748843\n",
       "50%      0.914315\n",
       "75%      0.964859\n",
       "max      0.979215\n",
       "Name: format_similarity, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[df_similarity[\"model\"] == \"gpt-4o\"][\"format_similarity\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
